{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0fcb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e01b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = pd.read_csv('train.csv') #need to split train_data into validation set later\n",
    "predict_set = pd.read_csv('test_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cedeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_tr.drop(df_tr[df_tr.MISSING_DATA == True].index)\n",
    "print(df_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797accf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 2, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15, where polyline_length = count(\"[\") - 1df_\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "\n",
    "# remove outliers\n",
    "df_tr = df_tr.drop(df_tr[df_tr.LEN <= 300].index)\n",
    "df_tr = df_tr.drop(df_tr[df_tr.LEN >= 10800].index)\n",
    "print(df_tr.shape)\n",
    "print(df_tr['LEN'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "from tzlocal import get_localzone\n",
    "from dataclasses import dataclass\n",
    "for df in (df_tr, predict_set): \n",
    "    df['start_time'] = pd.to_datetime(df['TIMESTAMP'], unit='s', utc=True)\n",
    "    df['start_time'] = df['start_time'].dt.tz_convert('GMT')\n",
    "    df['end_time'] = df['start_time'] + pd.to_timedelta(df_tr['LEN'], unit='s')\n",
    "    df['dt'] = pd.to_datetime(df['start_time'].dt.date)\n",
    "    df['t1'] = df['start_time'].dt.time\n",
    "    df['t2'] = df['end_time'].dt.time\n",
    "    df['day'] = df['start_time'].dt.dayofweek + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ba0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_tr\n",
    "import datetime\n",
    "del df_tr\n",
    "dates = pd.to_datetime(['2014-09-30', '2014-10-06', '2014-11-01', '2014-08-14', '2014-12-21'])\n",
    "\n",
    "s = datetime.time(7, 30)\n",
    "e = datetime.time(9, 30)\n",
    "train.loc[(train['day']>=1) & (train['day']<=5) & (train['t1']<e) & (train['t2']>s), 'dt'] = dates[0]\n",
    "\n",
    "s = datetime.time(16, 45)\n",
    "e = datetime.time(18, 45)\n",
    "train.loc[(train['day']>=1) & (train['day']<=5) & (train['t1']<e) & (train['t2']>s), 'dt'] = dates[1]\n",
    "\n",
    "s = datetime.time(2, 30)\n",
    "e = datetime.time(5, 0)\n",
    "train.loc[(train['day']==6) & (train['t1']<e) & (train['t2']>s), 'dt'] = dates[2]\n",
    "\n",
    "d = pd.to_datetime(['2013-08-14', '2013-12-24', '2014-04-17', '2014-04-24', '2014-04-30', '2014-06-09'])\n",
    "s = datetime.time(16, 0)\n",
    "e = datetime.time(20, 0)\n",
    "train.loc[train['dt'].isin(d) & (train['t1']<e) & (train['t2']>s), 'dt'] = dates[3]\n",
    "\n",
    "d = pd.to_datetime(['2013-08-11', '2013-12-08', '2013-12-22', '2013-12-29',  '2014-04-13', '2014-04-27', '2014-06-08'])\n",
    "s = datetime.time(13, 30)\n",
    "e = datetime.time(17, 15)\n",
    "train.loc[train['dt'].isin(d) & (train['t1']<e) & (train['t2']>s), 'dt'] = dates[4]\n",
    "\n",
    "train = train[train['dt'].isin(dates)]\n",
    "\n",
    "\n",
    "del d, e, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a27b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7e50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.to_datetime(['2014-12-21'])\n",
    "train[train['dt'].isin(d)].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7065bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['DAY_TYPE','MISSING_DATA','start_time','end_time','t1','t2','day'],axis=1)\n",
    "predict_set = predict_set.drop(['DAY_TYPE','MISSING_DATA','start_time','end_time','t1','t2','day'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c03fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac61ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb921f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=['dt'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e328ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_set = pd.get_dummies(predict_set,columns=['dt'])\n",
    "predict_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d543b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train=train.rename(columns={'dt_2014-08-14 00:00:00': 'dt_2014-08-14', 'dt_2014-09-30 00:00:00': 'dt_2014-09-30',\n",
    "                            'dt_2014-10-06 00:00:00': 'dt_2014-10-06', 'dt_2014-11-01 00:00:00': 'dt_2014-11-01',\n",
    "                            'dt_2014-12-21 00:00:00':'dt_2014-12-21'})\n",
    "train.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (train, predict_set):\n",
    "    df['TIMESTAMP'] = df['TIMESTAMP'].astype('datetime64[s]')\n",
    "    datetime_index = pd.DatetimeIndex(df['TIMESTAMP'])\n",
    "    df['QUARTER_HOUR'] = np.round(datetime_index.hour * 4 + datetime_index.minute / 15) % 96\n",
    "    # Extract day of week\n",
    "    df['DAY_OF_WEEK'] = datetime_index.dayofweek\n",
    "    # Extract week of year\n",
    "    df['WEEK_OF_YEAR'] = datetime_index.weekofyear - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_feature(feature, train, test):\n",
    "    \"\"\"\n",
    "    Encode the labels for the given feature across both the train and test datasets.\n",
    "    \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    train_values = train[feature].copy()\n",
    "    test_values = test[feature].copy()\n",
    "    # Replace missing values with 0's so we can later encode them\n",
    "    train_values[np.isnan(train_values)] = 0\n",
    "    test_values[np.isnan(test_values)] = 0\n",
    "    # Fit the labels across all possible values in both datasets\n",
    "    encoder.fit(pd.concat([train_values, test_values]))\n",
    "    # Add new column to the datasets with encoded values\n",
    "    train[feature + '_ENCODED'] = encoder.transform(train_values)\n",
    "    test[feature + '_ENCODED'] = encoder.transform(test_values)\n",
    "    return encoder\n",
    "client_encoder = encode_feature('ORIGIN_CALL', train, predict_set)\n",
    "taxi_encoder = encode_feature('TAXI_ID', train, predict_set)\n",
    "stand_encoder = encode_feature('ORIGIN_STAND', train, predict_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1fbb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93373be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'n_quarter_hours': 96,  # Number of quarter of hours in one day (i.e. 24 * 4).\n",
    "    'n_days_per_week': 7,\n",
    "    'n_weeks_per_year': 52,\n",
    "    'n_client_ids': len(client_encoder.classes_),\n",
    "    'n_taxi_ids': len(taxi_encoder.classes_),\n",
    "    'n_stand_ids': len(stand_encoder.classes_),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['LEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5732b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, train_labels, validation_labels = train_test_split(train, train_labels, test_size=0.02)\n",
    "validation, test, validation_labels, test_labels = train_test_split(validation, validation_labels, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451434d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cache = 'cache/train2.pickle'\n",
    "train_labels_cache = 'cache/train-labels2.npy'\n",
    "validation_cache = 'cache/validation2.pickle'\n",
    "validation_labels_cache = 'cache/validation-labels2.npy'\n",
    "test_cache = 'cache/test2.pickle'\n",
    "test_labels_cache = 'cache/test-labels2.npy'\n",
    "competition_test_cache = 'cache/competition-test2.pickle'\n",
    "metadata_cache = 'cache/metadata2.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e18cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle(train_cache)\n",
    "validation.to_pickle(validation_cache)\n",
    "test.to_pickle(test_cache)\n",
    "np.save(train_labels_cache, train_labels)\n",
    "np.save(validation_labels_cache, validation_labels)\n",
    "np.save(test_labels_cache, test_labels)\n",
    "predict_set.to_pickle(competition_test_cache)\n",
    "with open(metadata_cache, 'wb') as handle:\n",
    "    pickle.dump(metadata, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc2aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cache = 'cache/train2.pickle'\n",
    "train = pd.read_pickle(train_cache)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b461b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5aabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
