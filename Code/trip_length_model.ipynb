{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9381fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.optimizers import SGD, Adam, Adagrad\n",
    "#from keras import backend as K\n",
    "#from keras.layers import Embedding\n",
    "#from keras.layers import Dense, Reshape, Concatenate, Activation, Dropout\n",
    "#from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "train_cache = 'cache/train.pickle'\n",
    "train_labels_len_cache = 'cache/train-labels-len.npy'\n",
    "train_labels_dist_cache = 'cache/train-labels-dist.npy'\n",
    "\n",
    "validation_cache = 'cache/validation.pickle'\n",
    "validation_labels_len_cache = 'cache/validation-labels-len.npy'\n",
    "validation_labels_dist_cache = 'cache/validation-labels-dist.npy'\n",
    "\n",
    "test_cache = 'cache/test.pickle'\n",
    "test_labels_len_cache = 'cache/test-labels-len.npy'\n",
    "test_labels_dist_cache = 'cache/test-labels-dist.npy'\n",
    "\n",
    "competition_test_cache = 'cache/competition-test.pickle'\n",
    "metadata_cache = 'cache/metadata.pickle'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "061f8b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cache = 'cache/train.pickle'\n",
    "train_labels_cache = 'cache/train-labels.npy'\n",
    "validation_cache = 'cache/validation.pickle'\n",
    "validation_labels_cache = 'cache/validation-labels.npy'\n",
    "test_cache = 'cache/test.pickle'\n",
    "test_labels_cache = 'cache/test-labels.npy'\n",
    "competition_test_cache = 'cache/competition-test.pickle'\n",
    "metadata_cache = 'cache/metadata.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac1c275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(train_cache)\n",
    "validation = pd.read_pickle(validation_cache)\n",
    "test = pd.read_pickle(test_cache)\n",
    "\n",
    "train_labels = np.load(train_labels_cache)\n",
    "validation_labels = np.load(validation_labels_cache)\n",
    "test_labels = np.load(test_labels_cache)\n",
    "\n",
    "competition_test = pd.read_pickle(competition_test_cache)\n",
    "with open(metadata_cache, 'rb') as handle:\n",
    "    metadata = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b1f7e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def process_features(df):\n",
    "#    return torch.tensor(df[['QUARTER_HOUR','DAY_OF_WEEK','WEEK_OF_YEAR','ORIGIN_CALL_ENCODED','TAXI_ID_ENCODED','ORIGIN_STAND_ENCODED',\n",
    "#                           'STAND_LONGITUDE','STAND_LATITUDE']].values)\n",
    "def process_features(df):\n",
    "    return torch.tensor(df[['QUARTER_HOUR','DAY_OF_WEEK','WEEK_OF_YEAR','ORIGIN_CALL_ENCODED','TAXI_ID_ENCODED',\n",
    "                            'ORIGIN_STAND_ENCODED']].values)\n",
    "#def process_features(df):\n",
    "#    return torch.tensor(df[['QUARTER_HOUR','DAY_OF_WEEK','WEEK_OF_YEAR']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14432b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.embed_quarter_hour = nn.Embedding(metadata['n_quarter_hours'], 10)\n",
    "        self.embed_day_of_week = nn.Embedding(metadata['n_days_per_week'], 10)\n",
    "        self.embed_week_of_year = nn.Embedding(metadata['n_weeks_per_year'],10)\n",
    "        self.embed_client_ids = nn.Embedding(metadata['n_client_ids'],10)\n",
    "        self.embed_taxi_ids = nn.Embedding(metadata['n_taxi_ids'],10)\n",
    "        self.embed_stand_ids = nn.Embedding(metadata['n_stand_ids'],10)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_features=60, out_features=100),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(in_features = 100, out_features = 40),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(in_features = 40, out_features = 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.output_layer = nn.Linear(in_features=10, out_features=1)\n",
    "        #self.output_layer = nn.Linear(in_features=10, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qhr = self.embed_quarter_hour(x[:,0].to(torch.int32))\n",
    "        dow = self.embed_day_of_week(x[:,1].to(torch.int32))\n",
    "        woy = self.embed_week_of_year(x[:,2].to(torch.int32))\n",
    "        ci = self.embed_client_ids(x[:,3].to(torch.int32))\n",
    "        ti = self.embed_taxi_ids(x[:,4].to(torch.int32))\n",
    "        si =  self.embed_stand_ids(x[:,5].to(torch.int32))\n",
    "        #x = torch.cat([qhr,dow,woy,ci,ti,si,x[:,6:]],axis=1)\n",
    "        x = torch.cat([qhr,dow,woy,ci,ti,si],axis=1)\n",
    "        #x = torch.cat([qhr,dow,woy],axis=1)\n",
    "        x = x.to(torch.float32)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5960ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "learning_rate = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "num_epochs = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ae8513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = process_features(train)\n",
    "train_data = TensorDataset(train_data, torch.tensor(train_labels))\n",
    "validate_data = process_features(validation)\n",
    "validate_data = TensorDataset(validate_data,torch.tensor(validation_labels))\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(validate_data, batch_size = BATCH_SIZE,shuffle=True)\n",
    "#device = 'cpu'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "all_losses = []\n",
    "train_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3a6175a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9686c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():  # We don't need gradients for validation\n",
    "        for inputs, targets in val_loader:\n",
    "            # Move data to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Reshape targets\n",
    "            targets = torch.reshape(targets,(-1,1))\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = torch.sqrt(criterion(outputs, targets))  # RMSE\n",
    "\n",
    "            # Accumulate loss\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "    # Return average loss\n",
    "    average_val_loss = running_val_loss / len(val_loader)\n",
    "    return average_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9399e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(model, train_loader, val_loader, criterion, num_epochs):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr = learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i,(x, y) in enumerate(train_loader,0):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            y = torch.reshape(y,(-1,1))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = torch.sqrt(criterion(output, y))#RMSE\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        all_losses.append(epoch_loss/len(train_loader))\n",
    "        print(f\"Epoch: {epoch+1} Training Loss:{epoch_loss/len(train_loader)}\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "                y_val = torch.reshape(y_val,(-1,1))\n",
    "                preds = model(x_val)\n",
    "                val_loss += torch.sqrt(criterion(preds, y_val)).item() # RMSE\n",
    "        print(f\"Epoch: {epoch+1} Validation Loss:{val_loss/len(val_loader)}\")\n",
    "        PATH = f'model_state/model_epoch{epoch}.pth'     \n",
    "        torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a2cd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:486.7239877424236\n",
      "Epoch: 1 Validation Loss:467.8638427734375\n"
     ]
    }
   ],
   "source": [
    "train_data(model, train_loader, val_loader, criterion, 50) # use the training function you defined\n",
    "val_loss = validate(model, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf41a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_state/model_epoch26.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69baea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(torch.Tensor.cpu(torch.tensor(all_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79010bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_loss(model, test_set, test_label,criterion):\n",
    "    total_loss=0\n",
    "    test_dataset = TensorDataset(test_set, test_label)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y = torch.reshape(y,(-1,1))\n",
    "        output = model(x)\n",
    "        loss = torch.sqrt(criterion(output, y))#RMSE\n",
    "        total_loss += loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66fcbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(view_loss(model, process_features(test), torch.tensor(test_labels).to(torch.float32), criterion)/process_features(test).shape[0]*320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07150e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_input = torch.tensor(process_features(competition_test)).to(device)\n",
    "print(predict_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14bf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model):\n",
    "    predict_output = model(predict_input)\n",
    "    return predict_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91364a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mlp_predict = pd.read_csv('test_public.csv')\n",
    "embed_mlp_predict = embed_mlp_predict['TRIP_ID']\n",
    "predict_tensor = out.to('cpu').detach().numpy().flatten()\n",
    "embed_mlp_predict= pd.concat([embed_mlp_predict, pd.DataFrame(predict_tensor)], axis=1)\n",
    "embed_mlp_predict = embed_mlp_predict.rename(columns={0: 'TRAVEL_TIME'})\n",
    "embed_mlp_predict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5218f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mlp_predict.to_csv('Embedding_MLP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db9824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b9716",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = process_features(train)\n",
    "print(a[:,0])\n",
    "embedding1 = torch.nn.Embedding(96, 10)\n",
    "embedding2 = torch.nn.Embedding()\n",
    "print(embedding(a[0,0].to(torch.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b796ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = process_features(train)[0:90]\n",
    "embed_quarter_hour = nn.Embedding(metadata['n_quarter_hours'], 10)\n",
    "embed_day_of_week = nn.Embedding(metadata['n_days_per_week'], 10)\n",
    "embed_week_of_year = nn.Embedding(metadata['n_weeks_per_year'],10)\n",
    "embed_client_ids = nn.Embedding(metadata['n_client_ids'],10)\n",
    "embed_taxi_ids = nn.Embedding(metadata['n_taxi_ids'],10)\n",
    "embed_stand_ids = nn.Embedding(metadata['n_stand_ids'],10)\n",
    "qhr = embed_quarter_hour(x[:,0].to(torch.int32))\n",
    "dow = embed_day_of_week(x[:,1].to(torch.int32))\n",
    "woy = embed_week_of_year(x[:,2].to(torch.int32))\n",
    "ci = embed_client_ids(x[:,3].to(torch.int32))\n",
    "ti = embed_taxi_ids(x[:,4].to(torch.int32))\n",
    "si =  embed_stand_ids(x[:,5].to(torch.int32))\n",
    "x = torch.cat([qhr,dow,woy,ci,ti,si,x[:,6:]],axis=1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf63869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[0:91,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f293a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939afc14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f8dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52494422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(metadata):\n",
    "    \"\"\"\n",
    "    Creates all the layers for our neural network model.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Arbitrary dimension for all embeddings\n",
    "    embedding_dim = 10\n",
    "\n",
    "    # Quarter hour of the day embedding\n",
    "    embed_quarter_hour = Sequential()\n",
    "    embed_quarter_hour.add(Embedding(metadata['n_quarter_hours'], embedding_dim, input_length=1))\n",
    "    embed_quarter_hour.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Day of the week embedding\n",
    "    embed_day_of_week = Sequential()\n",
    "    embed_day_of_week.add(Embedding(metadata['n_days_per_week'], embedding_dim, input_length=1))\n",
    "    embed_day_of_week.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Week of the year embedding\n",
    "    embed_week_of_year = Sequential()\n",
    "    embed_week_of_year.add(Embedding(metadata['n_weeks_per_year'], embedding_dim, input_length=1))\n",
    "    embed_week_of_year.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Client ID embedding\n",
    "    embed_client_ids = Sequential()\n",
    "    embed_client_ids.add(Embedding(metadata['n_client_ids'], embedding_dim, input_length=1))\n",
    "    embed_client_ids.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Taxi ID embedding\n",
    "    embed_taxi_ids = Sequential()\n",
    "    embed_taxi_ids.add(Embedding(metadata['n_taxi_ids'], embedding_dim, input_length=1))\n",
    "    embed_taxi_ids.add(Reshape((embedding_dim,)))\n",
    "\n",
    "    # Taxi stand ID embedding\n",
    "    embed_stand_ids = Sequential()\n",
    "    embed_stand_ids.add(Embedding(metadata['n_stand_ids'], embedding_dim, input_length=1))\n",
    "    embed_stand_ids.add(Reshape((embedding_dim,)))\n",
    "    \n",
    "    # GPS coordinates (5 first lat/long and 5 latest lat/long, therefore 20 values)\n",
    "    coords = Sequential()\n",
    "    coords.add(Dense(1, input_dim=2))\n",
    "\n",
    "    # Merge all the inputs into a single input layer\n",
    "    model = Sequential()\n",
    "    preprocessing_layer = Concatenate([embed_quarter_hour, embed_day_of_week, embed_week_of_year,embed_client_ids,embed_taxi_ids,\n",
    "                                           embed_stand_ids,coords])\n",
    "    \n",
    "    model.add(preprocessing_layer)\n",
    "\n",
    "    # Simple hidden layer\n",
    "    model.add(Dense(200))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Determine cluster probabilities using softmax\n",
    "    model.add(Dense(50))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = SGD(lr=0.01, momentum=0.9, clipvalue=1.)  # Use `clipvalue` to prevent exploding gradients\n",
    "    model.compile(loss=tf.keras.metrics.RootMeanSquaredError(), optimizer=optimizer)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9086cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_new_session():\n",
    "    \"\"\"\n",
    "    Starts a new Tensorflow session.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure the session only uses the GPU memory that it actually needs\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    session = tf.compat.v1.Session(config=config, graph=tf.compat.v1.get_default_graph())\n",
    "    tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534bc543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(n_epochs=100, batch_size=200, save_prefix=None):\n",
    "    \n",
    "\n",
    "    # Set up callbacks\n",
    "    callbacks = []\n",
    "    if save_prefix is not None:\n",
    "        # Save the model's intermediary weights to disk after each epoch\n",
    "        file_path=\"cache/%s-{epoch:03d}-{val_loss:.4f}.hdf5\" % save_prefix\n",
    "        callbacks.append(ModelCheckpoint(file_path, monitor='val_loss', mode='min', save_weights_only=True, verbose=1))\n",
    "\n",
    "    # Create model\n",
    "    start_new_session()\n",
    "    model = create_model(metadata)\n",
    "    \n",
    "    # Run the training\n",
    "    history = model.fit(\n",
    "        process_features(train), train_labels,\n",
    "        epochs=n_epochs, batch_size=batch_size,\n",
    "        validation_data=(process_features(validation), validation_labels),\n",
    "        callbacks=callbacks)\n",
    "\n",
    "    if save_prefix is not None:\n",
    "        # Save the training history to disk\n",
    "        file_path = 'cache/%s-history.pickle' % save_prefix\n",
    "        with open(file_path, 'wb') as handle:\n",
    "            pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b692fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    " full_train(n_epochs=100, batch_size=200, save_prefix='mymodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038e5509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
